{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy_dataset = pd.read_csv('E:/ADS/Appliances-energy-prediction-data/energydata_complete.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy_dataset.rename(columns={'T1':'TempKitchen','RH_1':'HumKitchen','T2':'TempLivingRoom','RH_2':'HumLivingRoom',\n",
    "                   'T3':'TempLaundryRoom','RH_3':'HumLaundryRoom','T4':'TempOfficeRoom','RH_4':'HumOfficeRoom',\n",
    "                   'T5':'TempBathRoom','RH_5':'HumBathRoom','T6':'TempOutsideNorth','RH_6':'HumOutsideNorth',\n",
    "                   'T7':'TempIroningRoom','RH_7':'HumIroningRoom','T8':'TempTeenagerRoom','RH_8':'HumTeenagerRoom',\n",
    "                   'T9':'TempParentRoom','RH_9':'_HumParentRoom','T_out':'TempOutside','Press_mm_hg':'Pressure',\n",
    "                   'RH_out':'Humidity'},inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Converting date to datetime format\n",
    "energy_dataset['date'] = pd.to_datetime(energy_dataset['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Adding day\n",
    "energy_dataset['Day']=energy_dataset['date'].dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Adding month of the year\n",
    "energy_dataset['Month'] = energy_dataset['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Adding time of the day\n",
    "energy_dataset['hour'] = energy_dataset['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy_dataset['NSM'] = energy_dataset['hour']*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = energy_dataset[['lights', 'TempKitchen', 'HumKitchen', 'TempLivingRoom',\n",
    "      'HumLivingRoom', 'TempLaundryRoom', 'HumLaundryRoom', 'TempOfficeRoom',\n",
    "      'HumOfficeRoom', 'TempBathRoom', 'HumBathRoom',\n",
    "      'HumOutsideNorth', 'TempIroningRoom', 'HumIroningRoom',\n",
    "      'TempTeenagerRoom', 'HumTeenagerRoom', 'TempParentRoom',\n",
    "      'Pressure', 'Humidity', 'Windspeed',\n",
    "      'Tdewpoint']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = energy_dataset['Appliances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error =  53.8230484927\n",
      "Root Mean Square Error=  94.1046354021\n",
      "Root Sqaure =  0.163123816594\n",
      "Mape =  62.7602849458725\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Appliances</td>    <th>  R-squared:         </th> <td>   0.563</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.563</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5082.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 15 Mar 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:49:08</td>     <th>  Log-Likelihood:    </th> <td> -23537.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  3947</td>      <th>  AIC:               </th> <td>4.708e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  3946</td>      <th>  BIC:               </th> <td>4.708e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    1.0045</td> <td>    0.014</td> <td>   71.291</td> <td> 0.000</td> <td>    0.977</td> <td>    1.032</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2730.516</td> <th>  Durbin-Watson:     </th> <td>   2.020</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>36182.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>16.352</td>  <th>  Cond. No.          </th> <td>    1.00</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:             Appliances   R-squared:                       0.563\n",
       "Model:                            OLS   Adj. R-squared:                  0.563\n",
       "Method:                 Least Squares   F-statistic:                     5082.\n",
       "Date:                Thu, 15 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        22:49:08   Log-Likelihood:                -23537.\n",
       "No. Observations:                3947   AIC:                         4.708e+04\n",
       "Df Residuals:                    3946   BIC:                         4.708e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             1.0045      0.014     71.291      0.000       0.977       1.032\n",
       "==============================================================================\n",
       "Omnibus:                     2730.516   Durbin-Watson:                   2.020\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            36182.730\n",
       "Skew:                           3.230   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.352   Cond. No.                         1.00\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Linear Regression_Part4\n",
    "## Testing Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "y = list(y)\n",
    "prediction = lm.predict(X_test)\n",
    "r2 = r2_score(y_test, prediction)\n",
    "mse = mean_squared_error(y_test, prediction)\n",
    "mae = mean_absolute_error(y_test, prediction)\n",
    "rms = np.sqrt(mse)\n",
    "mape = np.mean ((np.abs((y_test - prediction) / y_test)) * 100)\n",
    "print(\"mean absolute error = \" , mae)\n",
    "#print(mse)\n",
    "print(\"Root Mean Square Error= \" , rms)\n",
    "print(\"Root Sqaure = \" ,  r2)\n",
    "print(\"Mape = \" , mape)\n",
    "model = sm.OLS(y_test,prediction)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error =  52.8547343085\n",
      "Root Mean Square Error=  94.0261013895\n",
      "Root Sqaure =  0.157440639499\n",
      "Mape =  60.7165709279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.558</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.558</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.993e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 15 Mar 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:49:25</td>     <th>  Log-Likelihood:    </th> <td> -94136.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 15788</td>      <th>  AIC:               </th> <td>1.883e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 15787</td>      <th>  BIC:               </th> <td>1.883e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    1.0000</td> <td>    0.007</td> <td>  141.184</td> <td> 0.000</td> <td>    0.986</td> <td>    1.014</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>11308.085</td> <th>  Durbin-Watson:     </th>  <td>   1.992</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>175565.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.367</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>17.884</td>   <th>  Cond. No.          </th>  <td>    1.00</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.558\n",
       "Model:                            OLS   Adj. R-squared:                  0.558\n",
       "Method:                 Least Squares   F-statistic:                 1.993e+04\n",
       "Date:                Thu, 15 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        22:49:25   Log-Likelihood:                -94136.\n",
       "No. Observations:               15788   AIC:                         1.883e+05\n",
       "Df Residuals:                   15787   BIC:                         1.883e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             1.0000      0.007    141.184      0.000       0.986       1.014\n",
       "==============================================================================\n",
       "Omnibus:                    11308.085   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           175565.709\n",
       "Skew:                           3.367   Prob(JB):                         0.00\n",
       "Kurtosis:                      17.884   Cond. No.                         1.00\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Linear Regression_Part4\n",
    "## Training Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "y = list(y)\n",
    "prediction = lm.predict(X_train)\n",
    "r2 = r2_score(y_train, prediction)\n",
    "mse = mean_squared_error(y_train, prediction)\n",
    "mae = mean_absolute_error(y_train, prediction)\n",
    "rms = np.sqrt(mse)\n",
    "mape = np.mean ((np.abs((y_train - prediction) / y_train)) * 100)\n",
    "print(\"mean absolute error = \" , mae)\n",
    "#print(mse)\n",
    "print(\"Root Mean Square Error= \" , rms)\n",
    "print(\"Root Sqaure = \" , r2)\n",
    "print(\"Mape = \" , mape)\n",
    "model = sm.OLS(y_train,prediction)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  42.3   58.8  240.  ...,  119.3   51.7   57.6]\n",
      "Mape 32.3965368842\n",
      "R2 0.533197665381\n",
      "MAE 32.5829237395\n",
      "RMSE 70.2824547858\n",
      "Accuracy testing : 0.533\n"
     ]
    }
   ],
   "source": [
    "## RandomForest_Part4\n",
    "## Testing Dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 100, random_state = np.random)\n",
    "#rf = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=123456)\n",
    "rf.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "prediction = rf.predict(X_test)\n",
    "r2 = r2_score(y_test, prediction)\n",
    "mse = mean_squared_error(y_test, prediction)\n",
    "mae = mean_absolute_error(y_test, prediction)\n",
    "#accuracy = accuracy_score(y_test, prediction)\n",
    "rmse = np.sqrt(mse)\n",
    "mape =  np.mean(np.abs((y_test-prediction) / y_test) ) * 100\n",
    "print(prediction)\n",
    "#print(\"Accuracy:\" , accuracy)\n",
    "print(\"Mape\" , mape)\n",
    "print(\"R2\" , r2)\n",
    "print(\"MAE\" , mae)\n",
    "print(\"RMSE\" , rmse)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "#accuracy = 100 - mape\n",
    "#print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "print('Accuracy testing : {:.3f}'.format(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape =  12.1492355012\n",
      "Root Sqaure =  0.934596671348\n",
      "Mean Absoulute Error =  12.3098365847\n",
      "Root Mean Square Error =  26.1967913686\n",
      "Accuracy training : 0.935\n"
     ]
    }
   ],
   "source": [
    "## RandomForest_Part4\n",
    "## Training Dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state = np.random)\n",
    "rf.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "prediction = rf.predict(X_train)\n",
    "r2 = r2_score(y_train, prediction)\n",
    "mse = mean_squared_error(y_train, prediction)\n",
    "mae = mean_absolute_error(y_train, prediction)\n",
    "mape = np.mean ((np.abs((y_train - prediction) / y_train)) * 100)\n",
    "#accuracy = 100 - mape\n",
    "rms = np.sqrt(mse)\n",
    "#print(predicted)\n",
    "print(\"Mape = \" , mape )\n",
    "#print(\"Accuracy = \" , accuracy)\n",
    "print(\"Root Sqaure = \" , r2)\n",
    "print(\"Mean Absoulute Error = \" , mae)\n",
    "print(\"Root Mean Square Error = \", rms)\n",
    "print('Accuracy training : {:.3f}'.format(rf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape 51.9714713658\n",
      "R2 0.117153679242\n",
      "MAE 48.2960730067\n",
      "RMSE 90.2154929195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,3947) and (24,100) not aligned: 3947 (dim 1) != 24 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-eb9f2a5f75f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MAE\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         return r2_score(y, self.predict(X), sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    387\u001b[0m                         multioutput='variance_weighted')\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \"\"\"\n\u001b[0;32m   1256\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coefs_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    674\u001b[0m                                          layer_units[i + 1])))\n\u001b[0;32m    675\u001b[0m         \u001b[1;31m# forward propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[1;32m--> 104\u001b[1;33m                                                  self.coefs_[i])\n\u001b[0m\u001b[0;32m    105\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,3947) and (24,100) not aligned: 3947 (dim 1) != 24 (dim 0)"
     ]
    }
   ],
   "source": [
    "##Neural Network\n",
    "## Testing Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,),  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
    "   learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "   random_state=0, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "   early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "mlp.fit(X_train,y_train)\n",
    "prediction = mlp.predict(X_test)\n",
    "r2 = r2_score(y_test, prediction)\n",
    "mse = mean_squared_error(y_test, prediction)\n",
    "mae = mean_absolute_error(y_test, prediction)\n",
    "#accuracy = accuracy_score(y_test, prediction)\n",
    "rmse = np.sqrt(mse)\n",
    "mape =  np.mean(np.abs((y_test-prediction) / y_test) ) * 100\n",
    "#print(prediction)\n",
    "#print(\"Accuracy:\" , accuracy)\n",
    "print(\"Mape\" , mape)\n",
    "print(\"R2\" , r2)\n",
    "print(\"MAE\" , mae)\n",
    "print(\"RMSE\" , rmse)\n",
    "accuracy = mlp.score(y_test, prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Neural Network\n",
    "## Training Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(30,30,30))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "mlp.fit(X_train,y_train)\n",
    "prediction = mlp.predict(X_train)\n",
    "r2 = r2_score(y_train, prediction)\n",
    "mse = mean_squared_error(y_train, prediction)\n",
    "mae = mean_absolute_error(y_train, prediction)\n",
    "#accuracy = accuracy_score(y_test, prediction)\n",
    "rmse = np.sqrt(mse)\n",
    "mape =  np.mean(np.abs((y_train-prediction) / y_train) ) * 100\n",
    "#print(prediction)\n",
    "#print(\"Accuracy:\" , accuracy)\n",
    "print(\"Mape\" , mape)\n",
    "print(\"R2\" , r2)\n",
    "print(\"MAE\" , mae)\n",
    "print(\"RMSE\" , rmse)\n",
    "accuracy = mlp.score(y_train, prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since Random Forest Regressor has the highest accuracy among all the algorithms, and with the highest r2 value we recommend to select random forest as the algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
